# -*- coding: utf-8 -*-
"""studymate.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Zw7S0JY0XkL80TwR7-TjkBXm09NnQ9YZ
"""

# StudyMate AI - PDF Q&A Application with IBM Granite-3.3-2B-Instruct
# Run this in Google Colab

# Install required packages
!pip install -q transformers torch accelerate bitsandbytes
!pip install -q gradio PyPDF2 sentence-transformers faiss-cpu
!pip install -q langchain langchain-community

import gradio as gr
import PyPDF2
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig
from sentence_transformers import SentenceTransformer
import faiss
import numpy as np
import re
from typing import List, Tuple
import io

class StudyMateApp:
    def __init__(self):
        self.tokenizer = None
        self.model = None
        self.embedding_model = None
        self.documents = []
        self.embeddings = None
        self.index = None
        self.setup_models()

    def setup_models(self):
        """Initialize the IBM Granite model and embedding model"""
        print("Loading IBM Granite-3.3-2B-Instruct model...")

        # Configure quantization for memory efficiency
        quantization_config = BitsAndBytesConfig(
            load_in_4bit=True,
            bnb_4bit_compute_dtype=torch.float16,
            bnb_4bit_quant_type="nf4",
            bnb_4bit_use_double_quant=True,
        )

        # Load IBM Granite model
        model_id = "ibm-granite/granite-3.3-2b-instruct"
        self.tokenizer = AutoTokenizer.from_pretrained(model_id)
        self.model = AutoModelForCausalLM.from_pretrained(
            model_id,
            quantization_config=quantization_config,
            device_map="auto",
            torch_dtype=torch.float16,
            trust_remote_code=True
        )

        # Load embedding model for document similarity
        print("Loading embedding model...")
        self.embedding_model = SentenceTransformer('all-MiniLM-L6-v2')

        print("Models loaded successfully!")

    def extract_text_from_pdf(self, pdf_file) -> Tuple[str, str]:
        """Extract text from uploaded PDF file"""
        try:
            pdf_reader = PyPDF2.PdfReader(pdf_file)
            text = ""
            filename = pdf_file.name

            for page_num, page in enumerate(pdf_reader.pages):
                page_text = page.extract_text()
                text += f"\n--- Page {page_num + 1} ---\n{page_text}\n"

            return text, filename
        except Exception as e:
            return f"Error extracting text: {str(e)}", pdf_file.name

    def chunk_text(self, text: str, chunk_size: int = 1000, overlap: int = 200) -> List[str]:
        """Split text into overlapping chunks"""
        chunks = []
        start = 0

        while start < len(text):
            end = start + chunk_size
            chunk = text[start:end]

            # Try to end at a sentence boundary
            if end < len(text):
                last_period = chunk.rfind('.')
                last_newline = chunk.rfind('\n')
                boundary = max(last_period, last_newline)
                if boundary > start + chunk_size // 2:
                    chunk = text[start:start + boundary + 1]
                    end = start + boundary + 1

            chunks.append(chunk.strip())
            start = end - overlap

            if start >= len(text):
                break

        return chunks

    def process_documents(self, files):
        """Process uploaded PDF files and create embeddings"""
        if not files:
            return "Please upload at least one PDF file."

        self.documents = []
        all_chunks = []

        try:
            for file in files:
                print(f"Processing {file.name}...")
                text, filename = self.extract_text_from_pdf(file)

                if text.startswith("Error"):
                    return f"Failed to process {filename}: {text}"

                # Split into chunks
                chunks = self.chunk_text(text)

                # Store chunks with metadata
                for i, chunk in enumerate(chunks):
                    self.documents.append({
                        'content': chunk,
                        'filename': filename,
                        'chunk_id': i
                    })
                    all_chunks.append(chunk)

            # Create embeddings
            print("Creating embeddings...")
            embeddings = self.embedding_model.encode(all_chunks)
            self.embeddings = np.array(embeddings)

            # Create FAISS index
            dimension = self.embeddings.shape[1]
            self.index = faiss.IndexFlatIP(dimension)
            # Normalize embeddings for cosine similarity
            faiss.normalize_L2(self.embeddings)
            self.index.add(self.embeddings)

            return f"Successfully processed {len(files)} PDF(s) with {len(all_chunks)} chunks."

        except Exception as e:
            return f"Error processing documents: {str(e)}"

    def retrieve_relevant_chunks(self, question: str, top_k: int = 5) -> List[dict]:
        """Retrieve most relevant document chunks for the question"""
        if self.index is None:
            return []

        # Encode question
        question_embedding = self.embedding_model.encode([question])
        question_embedding = np.array(question_embedding)
        faiss.normalize_L2(question_embedding)

        # Search similar chunks
        scores, indices = self.index.search(question_embedding, top_k)

        relevant_chunks = []
        for score, idx in zip(scores[0], indices[0]):
            if idx < len(self.documents):
                chunk_info = self.documents[idx].copy()
                chunk_info['relevance_score'] = float(score)
                relevant_chunks.append(chunk_info)

        return relevant_chunks

    def generate_answer(self, question: str, context_chunks: List[dict]) -> str:
        """Generate answer using IBM Granite model"""
        if not context_chunks:
            return "I couldn't find relevant information in the uploaded documents to answer your question."

        # Prepare context
        context = "\n\n".join([
            f"From {chunk['filename']} (Chunk {chunk['chunk_id']}):\n{chunk['content']}"
            for chunk in context_chunks[:3]  # Use top 3 chunks
        ])

        # Create prompt
        prompt = f"""Based on the following context from the uploaded documents, answer the question accurately and provide specific references.

Context:
{context}

Question: {question}

Answer: Provide a comprehensive answer based on the context above. Include references to the source documents and page information where possible."""

        # Tokenize and generate
        inputs = self.tokenizer(prompt, return_tensors="pt", truncation=True, max_length=2048)
        inputs = {k: v.to(self.model.device) for k, v in inputs.items()}

        with torch.no_grad():
            outputs = self.model.generate(
                **inputs,
                max_new_tokens=512,
                temperature=0.7,
                do_sample=True,
                pad_token_id=self.tokenizer.eos_token_id,
                repetition_penalty=1.1
            )

        # Decode response
        response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)

        # Extract only the generated answer
        answer_start = response.find("Answer:")
        if answer_start != -1:
            answer = response[answer_start + 7:].strip()
        else:
            answer = response[len(prompt):].strip()

        # Add source references
        sources = list(set([chunk['filename'] for chunk in context_chunks[:3]]))
        if sources:
            answer += f"\n\n**Sources:** {', '.join(sources)}"

        return answer

    def answer_question(self, question: str) -> str:
        """Main function to answer questions"""
        if not question.strip():
            return "Please enter a question."

        if not self.documents:
            return "Please upload and process PDF documents first."

        try:
            # Retrieve relevant chunks
            relevant_chunks = self.retrieve_relevant_chunks(question, top_k=5)

            if not relevant_chunks:
                return "I couldn't find relevant information in the uploaded documents to answer your question."

            # Generate answer
            answer = self.generate_answer(question, relevant_chunks)
            return answer

        except Exception as e:
            return f"Error generating answer: {str(e)}"

# Initialize the StudyMate application
studymate = StudyMateApp()

# Create Gradio interface
def create_interface():
    with gr.Blocks(title="StudyMate AI - PDF Q&A Assistant", theme=gr.themes.Soft()) as demo:
        gr.Markdown("""
        # üìö StudyMate AI - Your Intelligent Study Companion

        Upload your PDF documents (textbooks, notes, papers) and ask questions in natural language!
        Powered by IBM Granite-3.3-2B-Instruct model.
        """)

        with gr.Tab("üì§ Upload Documents"):
            with gr.Row():
                with gr.Column():
                    file_upload = gr.File(
                        label="Upload PDF Files",
                        file_count="multiple",
                        file_types=[".pdf"],
                        height=200
                    )
                    upload_btn = gr.Button("Process Documents", variant="primary", size="lg")

                with gr.Column():
                    upload_status = gr.Textbox(
                        label="Processing Status",
                        lines=5,
                        interactive=False,
                        placeholder="Upload PDF files and click 'Process Documents' to begin..."
                    )

        with gr.Tab("ü§ñ Ask Questions"):
            with gr.Row():
                with gr.Column(scale=2):
                    question_input = gr.Textbox(
                        label="Ask a Question",
                        placeholder="Example: What are the main causes of climate change?",
                        lines=3
                    )
                    ask_btn = gr.Button("Get Answer", variant="primary", size="lg")

                with gr.Column(scale=3):
                    answer_output = gr.Textbox(
                        label="StudyMate's Answer",
                        lines=15,
                        interactive=False,
                        placeholder="Your answer will appear here..."
                    )

        with gr.Tab("‚ÑπÔ∏è About"):
            gr.Markdown("""
            ## How StudyMate Works:

            1. **Upload PDFs**: Upload your study materials (textbooks, notes, research papers)
            2. **Process Documents**: The AI analyzes and indexes your documents
            3. **Ask Questions**: Ask questions in natural language
            4. **Get Answers**: Receive contextual answers with source references

            ## Features:
            - ‚úÖ Multiple PDF support
            - ‚úÖ Natural language Q&A
            - ‚úÖ Source references included
            - ‚úÖ Cross-document reasoning
            - ‚úÖ Powered by IBM Granite AI

            ## Tips for Better Results:
            - Upload clear, text-based PDFs
            - Ask specific, focused questions
            - Use complete sentences in your queries
            """)

        # Event handlers
        upload_btn.click(
            fn=studymate.process_documents,
            inputs=[file_upload],
            outputs=[upload_status]
        )

        ask_btn.click(
            fn=studymate.answer_question,
            inputs=[question_input],
            outputs=[answer_output]
        )

        # Allow Enter key to submit questions
        question_input.submit(
            fn=studymate.answer_question,
            inputs=[question_input],
            outputs=[answer_output]
        )

    return demo

# Launch the application
if __name__ == "__main__":
    print("üöÄ Launching StudyMate AI...")
    demo = create_interface()
    demo.launch(
        share=True,  # Creates public link
        debug=True,
        server_name="0.0.0.0",
        server_port=7860,
        show_error=True
    )

